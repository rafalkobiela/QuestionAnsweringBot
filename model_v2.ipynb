{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafal/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = np.load('question.npy')\n",
    "negative = np.load('Negative.npy')\n",
    "positive = np.load('Positive.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8484"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rando = np.arange(len(question))\n",
    "np.random.shuffle(rando)\n",
    "question,negative,positive = question[rando],negative[rando],positive[rando]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true,y_pred):\n",
    "    \n",
    "    \n",
    "    positive_pred = y_pred[:,1]\n",
    "    negative_pred = y_pred[:,0]\n",
    "    \n",
    "    sum_loss = 1 - positive_pred + negative_pred\n",
    "    \n",
    "    loss = K.maximum(sum_loss,0.0)\n",
    "    \n",
    "    return K.sum(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_log_softmax(x):\n",
    "    return K.log(K.softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIEMASE + TRIPLET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Perspective Sentence Similarity Modeling\n",
    "with Convolutional Neural Networks - part 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_input = Input(shape=(20,300), name = 'n')\n",
    "mid_input = Input(shape=(20,300), name = 'q')\n",
    "right_input = Input(shape=(20,300), name = 'p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = layers.Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Window size  = 3\n",
    "conv1_min_window_size_3 = layers.Conv1D(300,3, activation='tanh',\n",
    "                                        kernel_regularizer=regularizers.l2(1e-4),input_shape = (None,20,300))\n",
    "min_pool_1_size_3 = layers.GlobalMaxPooling1D()\n",
    "\n",
    "conv1_max_window_size_3 = layers.Conv1D(300,3, activation='tanh',\n",
    "                                        kernel_regularizer=regularizers.l2(1e-4),input_shape = (None,20,300))\n",
    "max_pool_1_size_3 = layers.GlobalMaxPooling1D()\n",
    "\n",
    "conv1_avg_window_size_3 = layers.Conv1D(300,3, activation='tanh',\n",
    "                                        kernel_regularizer=regularizers.l2(1e-4),input_shape = (None,20,300))\n",
    "avg_pool_1_size_3 = layers.GlobalAveragePooling1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Window size  = 2\n",
    "conv1_min_window_size_2 = layers.Conv1D(300,2, activation='tanh',\n",
    "                                        kernel_regularizer=regularizers.l2(1e-4),input_shape = (None,20,300))\n",
    "min_pool_1_size_2 = layers.GlobalMaxPooling1D()\n",
    "\n",
    "conv1_max_window_size_2 = layers.Conv1D(300,2, activation='tanh',\n",
    "                                        kernel_regularizer=regularizers.l2(1e-4),input_shape = (None,20,300))\n",
    "max_pool_1_size_2 = layers.GlobalMaxPooling1D()\n",
    "\n",
    "conv1_avg_window_size_2 = layers.Conv1D(300,2, activation='tanh',\n",
    "                                        kernel_regularizer=regularizers.l2(1e-4),input_shape = (None,20,300))\n",
    "avg_pool_1_size_2 = layers.GlobalAveragePooling1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Window size  = Infty = 20\n",
    "min_pool_1_size_20 = layers.GlobalMaxPooling1D()\n",
    "\n",
    "max_pool_1_size_20 = layers.GlobalMaxPooling1D()\n",
    "\n",
    "avg_pool_1_size_20 = layers.GlobalAveragePooling1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As there is no MinPooling in Keras, I use MaxPooling from - 1 * vector\n",
    "\n",
    "left_size3_max = max_pool_1_size_3(conv1_max_window_size_3(left_input))\n",
    "left_size3_min = min_pool_1_size_3(layers.Lambda(lambda x : x * -1 )(conv1_min_window_size_3(left_input)))\n",
    "left_size3_avg = avg_pool_1_size_3(conv1_avg_window_size_3(left_input))\n",
    "\n",
    "mid_size3_max = max_pool_1_size_3(conv1_max_window_size_3(mid_input))\n",
    "mid_size3_min = min_pool_1_size_3(layers.Lambda(lambda x : x * -1 )(conv1_min_window_size_3(mid_input)))\n",
    "mid_size3_avg = avg_pool_1_size_3(conv1_avg_window_size_3(mid_input))\n",
    "\n",
    "right_size3_max = max_pool_1_size_3(conv1_max_window_size_3(right_input))\n",
    "right_size3_min = min_pool_1_size_3(layers.Lambda(lambda x : x * -1 )(conv1_min_window_size_3(right_input)))\n",
    "right_size3_avg = avg_pool_1_size_3(conv1_avg_window_size_3(right_input))\n",
    "\n",
    "\n",
    "\n",
    "left_size2_max  = max_pool_1_size_2(conv1_max_window_size_2(left_input))\n",
    "left_size2_min = min_pool_1_size_2(layers.Lambda(lambda x : x * -1 )(conv1_min_window_size_2(left_input)))\n",
    "left_size2_avg = avg_pool_1_size_2(conv1_avg_window_size_2(left_input))\n",
    "\n",
    "mid_size2_max = max_pool_1_size_2(conv1_max_window_size_2(mid_input))\n",
    "mid_size2_min = min_pool_1_size_2(layers.Lambda(lambda x : x * -1 )(conv1_min_window_size_2(mid_input)))\n",
    "mid_size2_avg = avg_pool_1_size_2(conv1_avg_window_size_2(mid_input))\n",
    "\n",
    "right_size2_max = max_pool_1_size_2(conv1_max_window_size_3(right_input))\n",
    "right_size2_min = min_pool_1_size_2(layers.Lambda(lambda x : x * -1 )(conv1_min_window_size_3(right_input)))\n",
    "right_size2_avg = avg_pool_1_size_2(conv1_avg_window_size_3(right_input))\n",
    "\n",
    "\n",
    "\n",
    "left_sizeInf_max  = max_pool_1_size_20(left_input)\n",
    "left_sizeInf_min = min_pool_1_size_20(layers.Lambda(lambda x : x * -1 )(left_input))\n",
    "left_sizeInf_avg = avg_pool_1_size_20(left_input)\n",
    "\n",
    "mid_sizeInf_max = max_pool_1_size_20(mid_input)\n",
    "mid_sizeInf_min = min_pool_1_size_20(layers.Lambda(lambda x : x * -1 )(mid_input))\n",
    "mid_sizeInf_avg = avg_pool_1_size_20(mid_input)\n",
    "\n",
    "right_sizeInf_max = max_pool_1_size_20(right_input)\n",
    "right_sizeInf_min = min_pool_1_size_20(layers.Lambda(lambda x : x * -1 )(right_input))\n",
    "right_sizeInf_avg = avg_pool_1_size_20(right_input)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-dimension filters\n",
    "   \n",
    "per_dimension_layers_size_2_max = \\\n",
    "[[layers.Conv1D(3,2, activation='tanh',kernel_regularizer=regularizers.l2(1e-4),input_shape = (None,20,1))]\n",
    "for i in range(300)]\n",
    "\n",
    "\n",
    "per_dimension_layers_size_3_max = \\\n",
    "[[layers.Conv1D(3,3, activation='tanh',kernel_regularizer=regularizers.l2(1e-4),input_shape = (None,20,1))]\n",
    "for i in range(300)]\n",
    "\n",
    "\n",
    "per_dimension_layers_size_2_min = \\\n",
    "[[layers.Conv1D(3,2, activation='tanh',kernel_regularizer=regularizers.l2(1e-4),input_shape = (None,20,1))]\n",
    "for i in range(300)]\n",
    "\n",
    "\n",
    "per_dimension_layers_size_3_min = \\\n",
    "[[layers.Conv1D(3,3, activation='tanh',kernel_regularizer=regularizers.l2(1e-4),input_shape = (None,20,1))]\n",
    "for i in range(300)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per dimension filters cd\n",
    "\n",
    " \n",
    "left_single_dimension_max_size2 = [layers.GlobalMaxPooling1D()(\n",
    "    (per_dimension_layers_size_2_max[i][0](        \n",
    "    layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],3,1]) )(layers.Lambda(lambda x : x[:,:,i])(left_input))\n",
    "    ))) for i in range(300)]\n",
    "left_single_dimension_min_size2 = [layers.GlobalMaxPooling1D()(\n",
    "    (per_dimension_layers_size_2_min[i][0](layers.Lambda(lambda x : x * -1 )(        \n",
    "    layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],3,1]) )(layers.Lambda(lambda x : x[:,:,i])(left_input))\n",
    "    )))) for i in range(300)]\n",
    "\n",
    "left_single_dimension_max_size3 = [layers.GlobalMaxPooling1D()(\n",
    "    (per_dimension_layers_size_3_max[i][0](        \n",
    "    layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],3,1]) )(layers.Lambda(lambda x : x[:,:,i])(left_input))\n",
    "    ))) for i in range(300)]\n",
    "left_single_dimension_min_size3 = [layers.GlobalMaxPooling1D()(\n",
    "    (per_dimension_layers_size_3_min[i][0](layers.Lambda(lambda x : x * -1 )(        \n",
    "    layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],3,1]) )(layers.Lambda(lambda x : x[:,:,i])(left_input))\n",
    "    )))) for i in range(300)]\n",
    "\n",
    "\n",
    "right_single_dimension_max_size2 = [layers.GlobalMaxPooling1D()(\n",
    "    (per_dimension_layers_size_2_max[i][0](        \n",
    "    layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],3,1]) )(layers.Lambda(lambda x : x[:,:,i])(right_input))\n",
    "    ))) for i in range(300)]\n",
    "right_single_dimension_min_size2 = [layers.GlobalMaxPooling1D()(\n",
    "    (per_dimension_layers_size_2_min[i][0](layers.Lambda(lambda x : x * -1 )(        \n",
    "    layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],3,1]) )(layers.Lambda(lambda x : x[:,:,i])(right_input))\n",
    "    )))) for i in range(300)]\n",
    "\n",
    "right_single_dimension_max_size3 = [layers.GlobalMaxPooling1D()(\n",
    "    (per_dimension_layers_size_3_max[i][0](        \n",
    "    layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],3,1]) )(layers.Lambda(lambda x : x[:,:,i])(right_input))\n",
    "    ))) for i in range(300)]\n",
    "right_single_dimension_min_size3 = [layers.GlobalMaxPooling1D()(\n",
    "    (per_dimension_layers_size_3_min[i][0](layers.Lambda(lambda x : x * -1 )(        \n",
    "    layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],3,1]) )(layers.Lambda(lambda x : x[:,:,i])(right_input))\n",
    "    )))) for i in range(300)]\n",
    "\n",
    "\n",
    "mid_single_dimension_max_size2 = [layers.GlobalMaxPooling1D()(\n",
    "    (per_dimension_layers_size_2_max[i][0](        \n",
    "    layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],3,1]) )(layers.Lambda(lambda x : x[:,:,i])(mid_input))\n",
    "    ))) for i in range(300)]\n",
    "mid_single_dimension_min_size2 = [layers.GlobalMaxPooling1D()(\n",
    "    (per_dimension_layers_size_2_min[i][0](layers.Lambda(lambda x : x * -1 )(        \n",
    "    layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],3,1]) )(layers.Lambda(lambda x : x[:,:,i])(mid_input))\n",
    "    )))) for i in range(300)]\n",
    "\n",
    "mid_single_dimension_max_size3 = [layers.GlobalMaxPooling1D()(\n",
    "    (per_dimension_layers_size_3_max[i][0](        \n",
    "    layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],3,1]) )(layers.Lambda(lambda x : x[:,:,i])(mid_input))\n",
    "    ))) for i in range(300)]\n",
    "mid_single_dimension_min_size3 = [layers.GlobalMaxPooling1D()(\n",
    "    (per_dimension_layers_size_3_min[i][0](layers.Lambda(lambda x : x * -1 )(        \n",
    "    layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],3,1]) )(layers.Lambda(lambda x : x[:,:,i])(mid_input))\n",
    "    )))) for i in range(300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal comparison, AKA Algorithm 1 - cosine distance + L2 distance\n",
    "\n",
    "\n",
    "results_L = []\n",
    "for gather in ['avg','max','min']:\n",
    "  \n",
    "    results_L1 = [layers.dot([\n",
    "        layers.concatenate( [\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('left_size2_%s'%(gather)))),\n",
    "\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('left_size3_%s'%(gather)))),\n",
    "\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('left_sizeInf_%s'%(gather))))      \n",
    "      ]),\n",
    "        layers.concatenate( [\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('mid_size2_%s'%(gather)))),\n",
    "\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('mid_size3_%s'%(gather)))),\n",
    "\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('mid_sizeInf_%s'%(gather))))      \n",
    "      ])\n",
    "    ],axes = -1,normalize=True) for i in range(300)]\n",
    "\n",
    "\n",
    "    results_L2 = [layers.Lambda(lambda x : tf.norm(x,ord = 2,axis=-1,keepdims=True))\n",
    "        (layers.subtract([\n",
    "        layers.concatenate( [\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('left_size2_%s'%(gather)))),\n",
    "\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('left_size3_%s'%(gather)))),\n",
    "\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('left_sizeInf_%s'%(gather))))      \n",
    "        ]),\n",
    "        layers.concatenate( [\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('mid_size2_%s'%(gather)))),\n",
    "\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('mid_size3_%s'%(gather)))),\n",
    "\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('mid_sizeInf_%s'%(gather))))      \n",
    "      ])\n",
    "    ])) for i in range(300)]\n",
    "\n",
    "\n",
    "    results_L += results_L1\n",
    "    results_L += results_L2\n",
    "\n",
    "horizontal_left_compared = layers.concatenate(results_L)\n",
    "\n",
    "del results_L\n",
    "del results_L1\n",
    "del results_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal comparison, AKA Algorithm 1 - cosine distance + L2 distance\n",
    "\n",
    "\n",
    "results_R = []\n",
    "for gather in ['avg','max','min']:\n",
    "  \n",
    "    results_R1 = [layers.dot([\n",
    "        layers.concatenate( [\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('right_size2_%s'%(gather)))),\n",
    "\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('right_size3_%s'%(gather)))),\n",
    "\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('right_sizeInf_%s'%(gather))))      \n",
    "      ]),\n",
    "        layers.concatenate( [\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('mid_size2_%s'%(gather)))),\n",
    "\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('mid_size3_%s'%(gather)))),\n",
    "\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('mid_sizeInf_%s'%(gather))))      \n",
    "      ])\n",
    "    ],axes = -1,normalize=True) for i in range(300)]\n",
    "\n",
    "\n",
    "    results_R2 = [layers.Lambda(lambda x : tf.norm(x,ord = 2,axis=-1,keepdims=True))\n",
    "        (layers.subtract([\n",
    "        layers.concatenate( [\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('right_size2_%s'%(gather)))),\n",
    "\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('right_size3_%s'%(gather)))),\n",
    "\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('right_sizeInf_%s'%(gather))))      \n",
    "        ]),\n",
    "        layers.concatenate( [\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('mid_size2_%s'%(gather)))),\n",
    "\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('mid_size3_%s'%(gather)))),\n",
    "\n",
    "        layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1]) )\n",
    "        (layers.Lambda(lambda x : x[:,i])(eval('mid_sizeInf_%s'%(gather))))      \n",
    "      ])\n",
    "    ])) for i in range(300)]\n",
    "\n",
    "\n",
    "    results_R += results_R1\n",
    "    results_R += results_R2\n",
    "    \n",
    "horizontal_right_compared = layers.concatenate(results_R)\n",
    "\n",
    "del results_R\n",
    "del results_R1\n",
    "del results_R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vertical comparison,AKA Algorithm 2 - cosine distance + L2 distance + L1 distance\n",
    "\n",
    "v_results_L = []\n",
    "for gather in ['avg','max','min']:\n",
    "    for num,s1 in enumerate(['2','3','Inf']): \n",
    "        for s2 in ['2','3','Inf'][num:]:\n",
    "        \n",
    "            \n",
    "            v_results_L.append(layers.dot([\n",
    "                    eval('left_size%s_%s'%(s1,gather)),eval('mid_size%s_%s'%(s2,gather))\n",
    "                ],axes = -1,normalize=True))\n",
    "            v_results_L.append(layers.Lambda(lambda x : tf.norm(x,ord = 2,axis=-1,keepdims=True))\n",
    "                         (layers.subtract([\n",
    "                    eval('left_size%s_%s'%(s1,gather)),eval('mid_size%s_%s'%(s2,gather))])))\n",
    "            v_results_L.append(layers.Lambda(lambda x : tf.norm(x,ord = 1,axis=-1,keepdims=True))\n",
    "                         (layers.subtract([\n",
    "                    eval('left_size%s_%s'%(s1,gather)),eval('mid_size%s_%s'%(s2,gather))])))    \n",
    "\n",
    "    if gather != 'avg':\n",
    "        for s1 in ['2','3']:    \n",
    "            \n",
    "            v_results_L1 = [layers.dot([\n",
    "                layers.concatenate([(layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1])))\n",
    "                                (layers.Lambda(lambda x : x[:,i])(lay))\n",
    "                   for lay in eval('left_single_dimension_%s_size%s'%(gather,s1))]),\n",
    "                layers.concatenate([(layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1])))\n",
    "                                (layers.Lambda(lambda x : x[:,i])(lay))\n",
    "                   for lay in eval('mid_single_dimension_%s_size%s'%(gather,s1))])\n",
    "            ],axes = -1,normalize=True) for i in range(3)]\n",
    "\n",
    "            v_results_L2 = [layers.Lambda(lambda x : tf.norm(x,ord = 2,axis=-1,keepdims=True))\n",
    "                (layers.subtract([\n",
    "                layers.concatenate([(layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1])))\n",
    "                                (layers.Lambda(lambda x : x[:,i])(lay))\n",
    "                   for lay in eval('left_single_dimension_%s_size%s'%(gather,s1))]),\n",
    "                layers.concatenate([(layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1])))\n",
    "                                (layers.Lambda(lambda x : x[:,i])(lay))\n",
    "                   for lay in eval('mid_single_dimension_%s_size%s'%(gather,s1))])\n",
    "            ])) for i in range(3)]\n",
    "\n",
    "            v_results_L3 = [layers.Lambda(lambda x : tf.norm(x,ord = 1,axis=-1,keepdims=True))\n",
    "                (layers.subtract([\n",
    "                layers.concatenate([(layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1])))\n",
    "                                (layers.Lambda(lambda x : x[:,i])(lay))\n",
    "                   for lay in eval('left_single_dimension_%s_size%s'%(gather,s1))]),\n",
    "                layers.concatenate([(layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1])))\n",
    "                                (layers.Lambda(lambda x : x[:,i])(lay))\n",
    "                   for lay in eval('mid_single_dimension_%s_size%s'%(gather,s1))])\n",
    "            ])) for i in range(3)]\n",
    "            \n",
    "            v_results_L += v_results_L1\n",
    "            v_results_L += v_results_L2\n",
    "            v_results_L += v_results_L3\n",
    "            \n",
    "vertical_left_compared = layers.concatenate(v_results_L)\n",
    "\n",
    "del v_results_L\n",
    "del v_results_L1\n",
    "del v_results_L2\n",
    "del v_results_L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vertical comparison,AKA Algorithm 2 - cosine distance + L2 distance + L1 distance\n",
    "\n",
    "v_results_R = []\n",
    "for gather in ['avg','max','min']:\n",
    "    for num,s1 in enumerate(['2','3','Inf']): \n",
    "        for s2 in ['2','3','Inf'][num:]:\n",
    "        \n",
    "            \n",
    "            v_results_R.append(layers.dot([\n",
    "                    eval('right_size%s_%s'%(s1,gather)),eval('mid_size%s_%s'%(s2,gather))\n",
    "                ],axes = -1,normalize=True))\n",
    "            v_results_R.append(layers.Lambda(lambda x : tf.norm(x,ord = 2,axis=-1,keepdims=True))\n",
    "                         (layers.subtract([\n",
    "                    eval('right_size%s_%s'%(s1,gather)),eval('mid_size%s_%s'%(s2,gather))])))\n",
    "            v_results_R.append(layers.Lambda(lambda x : tf.norm(x,ord = 1,axis=-1,keepdims=True))\n",
    "                         (layers.subtract([\n",
    "                    eval('right_size%s_%s'%(s1,gather)),eval('mid_size%s_%s'%(s2,gather))])))    \n",
    "\n",
    "    if gather != 'avg':\n",
    "        for s1 in ['2','3']:    \n",
    "            \n",
    "            v_results_R1 = [layers.dot([\n",
    "                layers.concatenate([(layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1])))\n",
    "                                (layers.Lambda(lambda x : x[:,i])(lay))\n",
    "                   for lay in eval('right_single_dimension_%s_size%s'%(gather,s1))]),\n",
    "                layers.concatenate([(layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1])))\n",
    "                                (layers.Lambda(lambda x : x[:,i])(lay))\n",
    "                   for lay in eval('mid_single_dimension_%s_size%s'%(gather,s1))])\n",
    "            ],axes = -1,normalize=True) for i in range(3)]\n",
    "\n",
    "            v_results_R2 = [layers.Lambda(lambda x : tf.norm(x,ord = 2,axis=-1,keepdims=True))\n",
    "                (layers.subtract([\n",
    "                layers.concatenate([(layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1])))\n",
    "                                (layers.Lambda(lambda x : x[:,i])(lay))\n",
    "                   for lay in eval('right_single_dimension_%s_size%s'%(gather,s1))]),\n",
    "                layers.concatenate([(layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1])))\n",
    "                                (layers.Lambda(lambda x : x[:,i])(lay))\n",
    "                   for lay in eval('mid_single_dimension_%s_size%s'%(gather,s1))])\n",
    "            ])) for i in range(3)]\n",
    "\n",
    "            v_results_R3 = [layers.Lambda(lambda x : tf.norm(x,ord = 1,axis=-1,keepdims=True))\n",
    "                (layers.subtract([\n",
    "                layers.concatenate([(layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1])))\n",
    "                                (layers.Lambda(lambda x : x[:,i])(lay))\n",
    "                   for lay in eval('right_single_dimension_%s_size%s'%(gather,s1))]),\n",
    "                layers.concatenate([(layers.Lambda(lambda x : K.reshape(x,[K.shape(x)[0],1])))\n",
    "                                (layers.Lambda(lambda x : x[:,i])(lay))\n",
    "                   for lay in eval('mid_single_dimension_%s_size%s'%(gather,s1))])\n",
    "            ])) for i in range(3)]\n",
    "            \n",
    "            v_results_R += v_results_R1\n",
    "            v_results_R += v_results_R2\n",
    "            v_results_R += v_results_R3\n",
    "            \n",
    "vertical_right_compared = layers.concatenate(v_results_R)\n",
    "\n",
    "del v_results_R\n",
    "del v_results_R1\n",
    "del v_results_R2\n",
    "del v_results_R3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dense layers at the top of network\n",
    "\n",
    "dens1 = layers.Dense(256,activation='tanh',kernel_regularizer=regularizers.l2(1e-4))\n",
    "\n",
    "droprout1 = layers.Dropout(0.5)\n",
    "\n",
    "\n",
    "merged_L = droprout1(dens1(layers.concatenate([horizontal_left_compared,vertical_left_compared])))\n",
    "merged_R = droprout1(dens1(layers.concatenate([horizontal_right_compared,vertical_left_compared])))\n",
    "\n",
    "\n",
    "dens2 = layers.Dense(256, activation='tanh',kernel_regularizer=regularizers.l2(1e-4))\n",
    "dens_pred = layers.Dense(1, activation=my_log_softmax,kernel_regularizer=regularizers.l2(1e-4))\n",
    "\n",
    "pred_L = dens_pred(dens2(merged_L))\n",
    "pred_R = dens_pred(dens2(merged_R))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pred = layers.concatenate([pred_L,pred_R], axis = -1)\n",
    "\n",
    "model = Model(inputs = [left_input, mid_input, right_input], outputs = pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss=triplet_loss, \n",
    "              optimizer=optimizers.Adam(lr=0.001,clipnorm=1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit({'n' : negative, 'q' : question, 'p' : positive},\n",
    "          np.zeros(len(negative)), #anything, basicly, not importent for loss function\n",
    "          epochs=10, \n",
    "          batch_size = 128,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
